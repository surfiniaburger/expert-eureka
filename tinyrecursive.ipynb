{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os, re\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n!pip install transformers==4.55.4\n!pip install --no-deps trl==0.22.2\n\n!pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-14T00:53:07.757327Z","iopub.execute_input":"2025-10-14T00:53:07.757729Z","iopub.status.idle":"2025-10-14T00:53:47.694247Z","shell.execute_reply.started":"2025-10-14T00:53:07.757709Z","shell.execute_reply":"2025-10-14T00:53:47.693334Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\n\n# --- MEMORY OPTIMIZATIONS ---\nD_MODEL = 512\nN_LAYERS = 2\nN_SUP = 16\nN_RECURSION = 6\nT_RECURSION = 3\nBATCH_SIZE = 16  # HALVED from 32\nLEARNING_RATE = 2e-5\nDIM_FEEDFORWARD = 2048 # REDUCED from 2048 default\nNUM_EPOCHS = 20\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(f\"Batch Size set to: {BATCH_SIZE}\")\nprint(f\"Transformer Feed-Forward Dim set to: {DIM_FEEDFORWARD}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T00:56:23.081793Z","iopub.execute_input":"2025-10-14T00:56:23.082084Z","iopub.status.idle":"2025-10-14T00:56:23.088880Z","shell.execute_reply.started":"2025-10-14T00:56:23.082067Z","shell.execute_reply":"2025-10-14T00:56:23.088055Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nBatch Size set to: 16\nTransformer Feed-Forward Dim set to: 2048\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Batch 2: Model Definition (Tiny Recursive Model)\nHere, we define the architecture for our TinyRecursiveModel (TRM). According to the paper, this is a simple 2-layer Transformer-style network. It processes a concatenation of the input x, the current prediction y, and the latent reasoning state z. We'll create a standard Transformer encoder block for this.","metadata":{}},{"cell_type":"code","source":"class TinyRecursiveModel(nn.Module):\n    \"\"\"\n    Implements the Tiny Recursive Model (TRM) from the paper.\n    This is a small Transformer-based network with 2 layers.\n    \"\"\"\n    def __init__(self, d_model=D_MODEL, n_layers=N_LAYERS, nhead=8, dim_feedforward=2048, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n\n        # The paper's model takes 3 inputs (x, y, z). We'll concatenate them.\n        # An input projection layer to map the concatenated input to the model dimension.\n        self.input_proj = nn.Linear(d_model * 3, d_model)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n        # Output heads\n        # This head refines the latent state 'z'\n        self.latent_head = nn.Linear(d_model, d_model)\n        # This head refines the answer 'y'\n        self.answer_head = nn.Linear(d_model, d_model)\n\n    def forward(self, x, y, z):\n        \"\"\"\n        x: The embedded input question/puzzle. Shape: (batch, seq_len, d_model)\n        y: The current embedded prediction. Shape: (batch, seq_len, d_model)\n        z: The current latent reasoning state. Shape: (batch, seq_len, d_model)\n        \"\"\"\n        # Concatenate inputs along the feature dimension\n        combined_input = torch.cat((x, y, z), dim=-1)\n        \n        # Project the combined input to the model's dimension\n        projected_input = self.input_proj(combined_input)\n\n        # Pass through the Transformer encoder\n        transformer_output = self.transformer_encoder(projected_input)\n\n        # As per the paper's logic, the model can update both y and z.\n        # We'll create two separate outputs from the transformer result.\n        \n        # The logic is: given x, y, z -> produce a new z'\n        # And given y, z -> produce a new y'\n        \n        # For simplicity and following Figure 1, we will have two heads.\n        # The main output of the transformer will be used to update the latent 'z'\n        new_z = self.latent_head(transformer_output)\n\n        # The paper suggests the answer update step uses the (new) latent z and old y.\n        # We will model this by feeding the transformer output and old 'y' to the answer head.\n        new_y = self.answer_head(transformer_output + y) # Using a residual connection for stability\n\n        return new_y, new_z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T00:56:45.102142Z","iopub.execute_input":"2025-10-14T00:56:45.102762Z","iopub.status.idle":"2025-10-14T00:56:45.109876Z","shell.execute_reply.started":"2025-10-14T00:56:45.102735Z","shell.execute_reply":"2025-10-14T00:56:45.109105Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Batch 3: Defining the Recursive Logic and Training Functions\nThis is the core logic from the paper's pseudocode. We'll implement latent_recursion for the inner loop and deep_recursion for the outer loop, which cleverly uses torch.no_grad() to manage memory and mimic the paper's training strategy.","metadata":{}},{"cell_type":"code","source":"def latent_recursion(model, x, y, z, n=N_RECURSION):\n    \"\"\"\n    Performs the inner 'latent reasoning' loop.\n    'n' recursive steps to refine the latent state 'z'.\n    \"\"\"\n    for _ in range(n):\n        # In this simplified model, both y and z are returned.\n        # The paper's core idea is that the latent state 'z' is what's primarily refined here.\n        _, z = model(x, y, z)\n    \n    # After refining z, one final step to refine y\n    y, z = model(x, y, z)\n    \n    return y, z\n\ndef deep_recursion(model, x, y, z, n=N_RECURSION, T=T_RECURSION):\n    \"\"\"\n    Performs one step of the outer 'deep supervision' loop.\n    This involves T-1 steps with no gradients and one final step with gradients.\n    \"\"\"\n    # Recurse for T-1 steps without tracking gradients to save memory\n    with torch.no_grad():\n        for _ in range(T - 1):\n            y, z = latent_recursion(model, x, y, z, n)\n            \n    # Perform the final recursion step with gradients enabled\n    y, z = latent_recursion(model, x, y, z, n)\n    \n    return y, z\n\n# We also need an output head to convert our embedding back to vocabulary tokens\nclass OutputHead(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.linear = nn.Linear(d_model, vocab_size)\n\n    def forward(self, y_embedding):\n        return self.linear(y_embedding)\n\n# And a Q-head for the Adaptive Computation Time (ACT) halting mechanism\nclass QHead(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.linear = nn.Linear(d_model, 1)\n\n    def forward(self, y_embedding):\n        # We just need a single scalar value for the halt probability\n        # We take the mean over the sequence length dimension\n        return self.linear(y_embedding.mean(dim=1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T00:56:48.829779Z","iopub.execute_input":"2025-10-14T00:56:48.830262Z","iopub.status.idle":"2025-10-14T00:56:48.837260Z","shell.execute_reply.started":"2025-10-14T00:56:48.830236Z","shell.execute_reply":"2025-10-14T00:56:48.836553Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Batch 4: Data Preparation\nIn this batch, we will:\nLoad a Sudoku dataset from the Hugging Face Hub.\nCreate a simple tokenizer to convert the puzzle strings into numerical tokens.\nSet up a custom PyTorch Dataset to handle the tokenization.\nCreate DataLoaders to feed the data to our model in batches.","metadata":{}},{"cell_type":"code","source":"# --- DATA LOADING ---\ndataset_name = \"sapientinc/sudoku-extreme\"\ndataset = load_dataset(dataset_name)\n\n# Define Vocabulary\nVOCAB = ['.'] + [str(i) for i in range(1, 10)] # The dataset uses '.' for empty cells\nVOCAB_SIZE = len(VOCAB)\ntoken_to_id = {token: i for i, token in enumerate(VOCAB)}\nid_to_token = {i: token for i, token in enumerate(VOCAB)}\nSEQ_LEN = 81\n\n# Let's inspect the actual structure of the loaded dataset to be sure.\nprint(\"Dataset Features:\", dataset['train'].features)\nprint(\"\\nFirst Training Example:\", dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T00:56:52.379559Z","iopub.execute_input":"2025-10-14T00:56:52.379829Z","iopub.status.idle":"2025-10-14T00:56:53.385653Z","shell.execute_reply.started":"2025-10-14T00:56:52.379811Z","shell.execute_reply":"2025-10-14T00:56:53.384776Z"}},"outputs":[{"name":"stdout","text":"Dataset Features: {'source': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'rating': Value(dtype='int64', id=None)}\n\nFirst Training Example: {'source': 'puzzles4_forum_hardest_1905', 'question': '5...27..9..41......1..5.3...92.6.8...5......66..7..29.8...7...2.......8...9..36..', 'answer': '583427169974136528216859374792364851351298746648715293865971432137642985429583617', 'rating': 18}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass SudokuDataset(Dataset):\n    def __init__(self, data, token_map):\n        self.data = data\n        self.token_map = token_map\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # --- FINAL CORRECTION ---\n        # Using the exact column names you found: 'question' and 'answer'\n        # Also, the dataset uses '.' for empty cells, not '0'.\n        quiz_str = item['question']\n        solution_str = item['answer']\n        \n        # Convert strings to lists of integer token IDs\n        quiz_tokens = torch.tensor([self.token_map[char] for char in quiz_str], dtype=torch.long)\n        solution_tokens = torch.tensor([self.token_map[char] for char in solution_str], dtype=torch.long)\n        \n        return quiz_tokens, solution_tokens\n\n# Create the datasets.\n# The paper trains on only 1K samples, so let's use .select() for efficiency.\ntrain_data = SudokuDataset(dataset['train'].select(range(1000)), token_to_id)\ntest_data = SudokuDataset(dataset['test'].select(range(1000)), token_to_id) # Using a 1k slice of test for quick eval\n\n# Create the DataLoaders\ntrain_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n# Let's inspect a single batch to confirm the shape\ntry:\n    sample_quiz, sample_solution = next(iter(train_dataloader))\n    print(f\"\\nQuiz batch shape: {sample_quiz.shape}\")\n    print(f\"Solution batch shape: {sample_solution.shape}\")\nexcept Exception as e:\n    print(f\"\\nFailed to create a batch. Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T00:56:57.164929Z","iopub.execute_input":"2025-10-14T00:56:57.165226Z","iopub.status.idle":"2025-10-14T00:56:57.187544Z","shell.execute_reply.started":"2025-10-14T00:56:57.165206Z","shell.execute_reply":"2025-10-14T00:56:57.186901Z"}},"outputs":[{"name":"stdout","text":"\nQuiz batch shape: torch.Size([16, 81])\nSolution batch shape: torch.Size([16, 81])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Batch 5: Initializing Models and Optimizer\nNow we have our model architecture and our data loaders. The final step before training is to instantiate all the necessary components:\nEmbedding Layer: A layer to convert our numerical tokens into dense vectors (the d_model dimension).\nTRM Model: Our main recursive network.\nOutput Head: To convert the model's output embeddings back into token probabilities.\nQ-Head: For the halting mechanism.\nOptimizer: To update the model's weights during training.\nLoss Function: To measure the difference between the model's predictions and the true solutions.","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# : Login to Hugging Face and Weights & Biases\n# ==============================================================================\nimport wandb\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# --- Hugging Face Login ---\nprint(\"--- Attempting Hugging Face Login ---\")\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n    login(token=hf_token)\n    print(\"✅ Successfully logged into Hugging Face.\")\nexcept Exception as e:\n    print(\"Could not log into Hugging Face. Please ensure the 'HUGGINGFACE_API_KEY' secret is set.\")\n    print(f\"Error: {e}\")\n\n# --- Weights & Biases Login ---\nprint(\"\\n--- Attempting Weights & Biases Login ---\")\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n    wandb.login(key=wandb_api_key)\n    print(\"✅ Successfully logged into Weights & Biases.\")\n    \n    # --- Initialize W&B Run ---\n    # This should happen right after a successful login\n    run = wandb.init(\n        project=\"tiny-recursive-model-sudoku-v1\",\n        config={\n            \"learning_rate\": LEARNING_RATE,\n            \"epochs\": NUM_EPOCHS,\n            \"batch_size\": BATCH_SIZE,\n            \"d_model\": D_MODEL,\n            \"n_layers\": N_LAYERS,\n            \"dim_feedforward\": DIM_FEEDFORWARD,\n            \"n_recursion\": N_RECURSION,\n            \"t_recursion\": T_RECURSION,\n        },\n    )\n    print(\"✅ W&B run initialized successfully.\")\n\nexcept Exception as e:\n    print(\"Could not log into W&B or initialize run. Please ensure the 'wandb_api_key' secret is set.\")\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T00:57:00.924942Z","iopub.execute_input":"2025-10-14T00:57:00.925242Z","iopub.status.idle":"2025-10-14T00:57:07.755435Z","shell.execute_reply.started":"2025-10-14T00:57:00.925221Z","shell.execute_reply":"2025-10-14T00:57:07.754786Z"}},"outputs":[{"name":"stdout","text":"--- Attempting Hugging Face Login ---\n✅ Successfully logged into Hugging Face.\n\n--- Attempting Weights & Biases Login ---\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"✅ Successfully logged into Weights & Biases.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251014_005701-cnmaputq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq' target=\"_blank\">brisk-sea-1</a></strong> to <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq</a>"},"metadata":{}},{"name":"stdout","text":"✅ W&B run initialized successfully.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 1. Token Embedding Layer\nembedding_layer = nn.Embedding(VOCAB_SIZE, D_MODEL).to(device)\n\n# 2. The main Tiny Recursive Model\n# --- MEMORY OPTIMIZATION ---\n# Pass our smaller dim_feedforward to the model constructor\ntrm_model = TinyRecursiveModel(\n    d_model=D_MODEL,\n    n_layers=N_LAYERS,\n    dim_feedforward=DIM_FEEDFORWARD\n).to(device)\n\n# 3. The Output Head\noutput_head = OutputHead(D_MODEL, VOCAB_SIZE).to(device)\n\n# 4. The Q-Head\nq_head = QHead(D_MODEL).to(device)\n\n# Combine all model parameters for the optimizer\nall_params = (\n    list(embedding_layer.parameters()) +\n    list(trm_model.parameters()) +\n    list(output_head.parameters()) +\n    list(q_head.parameters())\n)\n\n# 5. Optimizer\noptimizer = AdamW(all_params, lr=LEARNING_RATE)\n\n# 6. Loss Functions\nprediction_loss_fn = nn.CrossEntropyLoss()\nact_loss_fn = nn.BCEWithLogitsLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T00:57:15.257646Z","iopub.execute_input":"2025-10-14T00:57:15.257909Z","iopub.status.idle":"2025-10-14T00:57:15.328092Z","shell.execute_reply.started":"2025-10-14T00:57:15.257891Z","shell.execute_reply":"2025-10-14T00:57:15.327548Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"Batch 6: The Training Loop\nThis is where we put everything together. The loop will iterate through our training data for a few epochs. Inside the loop, we implement the full \"Deep Supervision\" logic as described in the paper's pseudocode.\nInitialize States: For each puzzle, we start with initial y and z embeddings. We'll use simple zero tensors for this.\nDeep Supervision Loop: We loop for N_SUP steps.\nForward Pass: In each step, we call our deep_recursion function.\nCalculate Losses:\nCalculate the prediction loss between the model's output and the true solution.\nCalculate the ACT (halting) loss. The paper's pseudocode suggests the target is 1 if the prediction is correct and 0 otherwise.\nBackpropagation: We compute the gradients and update the model weights.\nDetach: Crucially, we detach the y and z states from the computation graph before the next supervision step. This is the key to managing memory.\nACT Halting: We check the halt condition from the Q-head to potentially break the loop early, saving computation.","metadata":{}},{"cell_type":"code","source":"# --- DEEPER TRAINING SETUP with W&B ---\n# NUM_EPOCHS = 2\n\n# We need to tell W&B to watch our model for gradient tracking (optional but good practice)\nwandb.watch(trm_model, log=\"all\", log_freq=100)\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n    \n    # --- TRAINING PHASE ---\n    trm_model.train() # Set model to training mode\n    epoch_loss = 0.0\n    progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")\n    \n    for i, (quiz_tokens, solution_tokens) in enumerate(progress_bar):\n        quiz_tokens = quiz_tokens.to(device)\n        solution_tokens = solution_tokens.to(device)\n        \n        optimizer.zero_grad()\n        \n        x_embedded = embedding_layer(quiz_tokens)\n        y_embedded = torch.zeros_like(x_embedded, device=device)\n        z_embedded = torch.zeros_like(x_embedded, device=device)\n\n        total_loss = 0\n        actual_steps = 0\n\n        for step in range(N_SUP):\n            actual_steps += 1\n            y_refined, z_refined = deep_recursion(trm_model, x_embedded, y_embedded, z_embedded)\n            \n            y_logits = output_head(y_refined)\n            q_value = q_head(y_refined)\n\n            prediction_loss = prediction_loss_fn(y_logits.view(-1, VOCAB_SIZE), solution_tokens.view(-1))\n\n            with torch.no_grad():\n                y_pred_tokens = torch.argmax(y_logits, dim=-1)\n                is_correct = torch.all(y_pred_tokens == solution_tokens, dim=1).float()\n            \n            halt_loss = act_loss_fn(q_value.squeeze(), is_correct)\n            step_loss = prediction_loss + halt_loss\n            total_loss += step_loss\n\n            y_embedded = y_refined.detach()\n            z_embedded = z_refined.detach()\n\n            if torch.sigmoid(q_value).mean() > 0.9 and step > 0:\n                break\n        \n        avg_loss = total_loss / actual_steps\n        avg_loss.backward()\n        optimizer.step()\n        \n        epoch_loss += avg_loss.item()\n        \n        # Log batch loss to W&B\n        wandb.log({\"batch_loss\": avg_loss.item()})\n        progress_bar.set_postfix({\"avg_loss\": epoch_loss / (i + 1)})\n\n    avg_epoch_loss = epoch_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1} finished. Average Training Loss: {avg_epoch_loss}\")\n    \n    # --- EVALUATION PHASE ---\ndef evaluate(embedding_layer, model, output_head, test_loader, device):\n    embedding_layer.eval()\n    model.eval()\n    output_head.eval()\n    q_head.eval() # Also set q_head to eval mode\n\n    total_puzzles = 0\n    correct_puzzles = 0\n    total_digits = 0\n    correct_digits = 0\n\n    with torch.no_grad():\n        for quiz_tokens, solution_tokens in tqdm(test_loader, desc=\"Evaluating\"):\n            quiz_tokens = quiz_tokens.to(device)\n            solution_tokens = solution_tokens.to(device)\n\n            # --- Evaluation follows the same deep recursion logic ---\n            x_embedded = embedding_layer(quiz_tokens)\n            y_embedded = torch.zeros_like(x_embedded, device=device)\n            z_embedded = torch.zeros_like(x_embedded, device=device)\n            \n            final_y_pred_tokens = None\n\n            # The paper uses the full N_sup steps at test time\n            for step in range(N_SUP):\n                y_refined, z_refined = deep_recursion(model, x_embedded, y_embedded, z_embedded)\n                y_embedded, z_embedded = y_refined.detach(), z_refined.detach()\n                \n                # We can check the final prediction after all steps\n                if step == N_SUP - 1:\n                    y_logits = output_head(y_refined)\n                    final_y_pred_tokens = torch.argmax(y_logits, dim=-1)\n\n            # Compare the final prediction with the solution\n            correct_puzzles_batch = torch.all(final_y_pred_tokens == solution_tokens, dim=1).sum().item()\n            correct_digits_batch = (final_y_pred_tokens == solution_tokens).sum().item()\n            \n            correct_puzzles += correct_puzzles_batch\n            total_puzzles += quiz_tokens.size(0)\n            correct_digits += correct_digits_batch\n            total_digits += quiz_tokens.size(0) * SEQ_LEN\n\n    puzzle_accuracy = (correct_puzzles / total_puzzles) * 100\n    digit_accuracy = (correct_digits / total_digits) * 100\n    \n    return puzzle_accuracy, digit_accuracy\n    \n    print(\"Running evaluation...\")\n    puzzle_acc, digit_acc = evaluate(embedding_layer, trm_model, output_head, test_dataloader, device)\n    \n    print(f\"Evaluation Results - Puzzle Acc: {puzzle_acc:.2f}%, Digit Acc: {digit_acc:.2f}%\")\n\n    # Log epoch-level metrics to W&B\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"avg_train_loss\": avg_epoch_loss,\n        \"puzzle_accuracy\": puzzle_acc,\n        \"digit_accuracy\": digit_acc,\n    })\n\nprint(\"Training finished!\")\n\n# --- FINISH W&B RUN ---\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T01:04:06.159818Z","iopub.execute_input":"2025-10-14T01:04:06.160422Z","iopub.status.idle":"2025-10-14T01:14:30.195763Z","shell.execute_reply.started":"2025-10-14T01:04:06.160399Z","shell.execute_reply":"2025-10-14T01:14:30.194944Z"}},"outputs":[{"name":"stdout","text":"--- Epoch 1/2 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 63/63 [05:11<00:00,  4.94s/it, avg_loss=1.53]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 finished. Average Training Loss: 1.5275759034686618\n--- Epoch 2/2 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 63/63 [05:12<00:00,  4.95s/it, avg_loss=1.51]","output_type":"stream"},{"name":"stdout","text":"Epoch 2 finished. Average Training Loss: 1.5147268147695632\nTraining finished!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▇▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>1.4997</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">brisk-sea-1</strong> at: <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq</a><br> View project at: <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251014_005701-cnmaputq/logs</code>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"NUM_EPOCHS = 3 # Let's start with 3 epochs for a quick test run.\n\n# Set models to training mode\nembedding_layer.train()\ntrm_model.train()\noutput_head.train()\nq_head.train()\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n    \n    # Use tqdm for a progress bar\n    for quiz_tokens, solution_tokens in tqdm(train_dataloader, desc=\"Training\"):\n        quiz_tokens = quiz_tokens.to(device)\n        solution_tokens = solution_tokens.to(device)\n        \n        # --- Start of Deep Supervision ---\n        \n        # 1. Initialize states for the deep supervision loop\n        # Embed the input puzzle once per supervision cycle\n        x_embedded = embedding_layer(quiz_tokens)\n\n        # Initialize y (prediction) and z (latent) embeddings as zero tensors\n        # The paper calls this y_init and z_init\n        y_embedded = torch.zeros_like(x_embedded, device=device)\n        z_embedded = torch.zeros_like(x_embedded, device=device)\n\n        # Reset gradients for the new supervision cycle\n        optimizer.zero_grad()\n        \n        total_loss = 0\n\n        for step in range(N_SUP):\n            # 2. Perform one full deep recursion step (T-1 no-grad, 1 with-grad)\n            y_refined, z_refined = deep_recursion(trm_model, x_embedded, y_embedded, z_embedded)\n            \n            # Get the model's prediction logits and halting value\n            y_logits = output_head(y_refined)\n            q_value = q_head(y_refined) # Halting probability logit\n\n            # 3. Calculate Losses\n            # Reshape for CrossEntropyLoss: (Batch * SeqLen, VocabSize)\n            prediction_loss = prediction_loss_fn(\n                y_logits.view(-1, VOCAB_SIZE),\n                solution_tokens.view(-1)\n            )\n\n            # ACT halting loss\n            with torch.no_grad():\n                # Get the actual predictions by finding the max logit\n                y_pred_tokens = torch.argmax(y_logits, dim=-1)\n                # The halt target is 1 if the entire puzzle is correct, 0 otherwise.\n                is_correct = torch.all(y_pred_tokens == solution_tokens, dim=1).float()\n            \n            # The paper's pseudocode uses a simple BCE loss for halting\n            halt_loss = act_loss_fn(q_value.squeeze(), is_correct)\n\n            # Combine losses\n            step_loss = prediction_loss + halt_loss\n            total_loss += step_loss\n\n            # 4. Detach states for the next iteration (THIS IS THE KEY MEMORY SAVING STEP)\n            y_embedded = y_refined.detach()\n            z_embedded = z_refined.detach()\n\n            # 5. ACT Early Stopping\n            # If the model is confident enough to halt, we stop the supervision loop for this batch.\n            # We use sigmoid to convert the logit to a probability\n            if torch.sigmoid(q_value).mean() > 0.9 and step > 0: # Check if average halt prob is high\n                break\n        \n        # 6. Backpropagation\n        # The accumulated loss from all steps is backpropagated at once.\n        total_loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch+1} finished. Last batch total loss: {total_loss.item()}\")\n\nprint(\"Training finished!\")","metadata":{"execution":{"iopub.status.busy":"2025-10-14T00:17:23.928306Z","iopub.execute_input":"2025-10-14T00:17:23.928716Z","iopub.status.idle":"2025-10-14T00:32:22.391271Z","shell.execute_reply.started":"2025-10-14T00:17:23.928695Z","shell.execute_reply":"2025-10-14T00:32:22.390349Z"}}},{"cell_type":"markdown","source":"def evaluate(embedding_layer, model, output_head, test_loader, device):\n    embedding_layer.eval()\n    model.eval()\n    output_head.eval()\n    q_head.eval() # Also set q_head to eval mode\n\n    total_puzzles = 0\n    correct_puzzles = 0\n    total_digits = 0\n    correct_digits = 0\n\n    with torch.no_grad():\n        for quiz_tokens, solution_tokens in tqdm(test_loader, desc=\"Evaluating\"):\n            quiz_tokens = quiz_tokens.to(device)\n            solution_tokens = solution_tokens.to(device)\n\n            # --- Evaluation follows the same deep recursion logic ---\n            x_embedded = embedding_layer(quiz_tokens)\n            y_embedded = torch.zeros_like(x_embedded, device=device)\n            z_embedded = torch.zeros_like(x_embedded, device=device)\n            \n            final_y_pred_tokens = None\n\n            # The paper uses the full N_sup steps at test time\n            for step in range(N_SUP):\n                y_refined, z_refined = deep_recursion(model, x_embedded, y_embedded, z_embedded)\n                y_embedded, z_embedded = y_refined.detach(), z_refined.detach()\n                \n                # We can check the final prediction after all steps\n                if step == N_SUP - 1:\n                    y_logits = output_head(y_refined)\n                    final_y_pred_tokens = torch.argmax(y_logits, dim=-1)\n\n            # Compare the final prediction with the solution\n            correct_puzzles_batch = torch.all(final_y_pred_tokens == solution_tokens, dim=1).sum().item()\n            correct_digits_batch = (final_y_pred_tokens == solution_tokens).sum().item()\n            \n            correct_puzzles += correct_puzzles_batch\n            total_puzzles += quiz_tokens.size(0)\n            correct_digits += correct_digits_batch\n            total_digits += quiz_tokens.size(0) * SEQ_LEN\n\n    puzzle_accuracy = (correct_puzzles / total_puzzles) * 100\n    digit_accuracy = (correct_digits / total_digits) * 100\n    \n    return puzzle_accuracy, digit_accuracy\n\n# --- Run Evaluation ---\nprint(\"\\nStarting evaluation on the test set...\")\npuzzle_acc, digit_acc = evaluate(embedding_layer, trm_model, output_head, test_dataloader, device)\n\nprint(f\"\\n--- Evaluation Results ---\")\nprint(f\"Puzzle Accuracy: {puzzle_acc:.2f}%\")\nprint(f\"Digit Accuracy: {digit_acc:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2025-10-14T00:32:22.392197Z","iopub.execute_input":"2025-10-14T00:32:22.392477Z","iopub.status.idle":"2025-10-14T00:35:13.439134Z","shell.execute_reply.started":"2025-10-14T00:32:22.392453Z","shell.execute_reply":"2025-10-14T00:35:13.438305Z"}}}]}