{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-14T00:53:07.757729Z",
     "iopub.status.busy": "2025-10-14T00:53:07.757327Z",
     "iopub.status.idle": "2025-10-14T00:53:47.694247Z",
     "shell.execute_reply": "2025-10-14T00:53:47.693334Z",
     "shell.execute_reply.started": "2025-10-14T00:53:07.757709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.55.4\n",
    "!pip install --no-deps trl==0.22.2\n",
    "\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:56:23.082084Z",
     "iopub.status.busy": "2025-10-14T00:56:23.081793Z",
     "iopub.status.idle": "2025-10-14T00:56:23.088880Z",
     "shell.execute_reply": "2025-10-14T00:56:23.088055Z",
     "shell.execute_reply.started": "2025-10-14T00:56:23.082067Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch Size set to: 16\n",
      "Transformer Feed-Forward Dim set to: 2048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- MEMORY OPTIMIZATIONS ---\n",
    "D_MODEL = 512\n",
    "N_LAYERS = 2\n",
    "N_SUP = 16\n",
    "N_RECURSION = 6\n",
    "T_RECURSION = 3\n",
    "BATCH_SIZE = 16  # HALVED from 32\n",
    "LEARNING_RATE = 2e-5\n",
    "DIM_FEEDFORWARD = 2048 # REDUCED from 2048 default\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Batch Size set to: {BATCH_SIZE}\")\n",
    "print(f\"Transformer Feed-Forward Dim set to: {DIM_FEEDFORWARD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 2: Model Definition (Tiny Recursive Model)\n",
    "Here, we define the architecture for our TinyRecursiveModel (TRM). According to the paper, this is a simple 2-layer Transformer-style network. It processes a concatenation of the input x, the current prediction y, and the latent reasoning state z. We'll create a standard Transformer encoder block for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:56:45.102762Z",
     "iopub.status.busy": "2025-10-14T00:56:45.102142Z",
     "iopub.status.idle": "2025-10-14T00:56:45.109876Z",
     "shell.execute_reply": "2025-10-14T00:56:45.109105Z",
     "shell.execute_reply.started": "2025-10-14T00:56:45.102735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TinyRecursiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Tiny Recursive Model (TRM) from the paper.\n",
    "    This is a small Transformer-based network with 2 layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=D_MODEL, n_layers=N_LAYERS, nhead=8, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # The paper's model takes 3 inputs (x, y, z). We'll concatenate them.\n",
    "        # An input projection layer to map the concatenated input to the model dimension.\n",
    "        self.input_proj = nn.Linear(d_model * 3, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Output heads\n",
    "        # This head refines the latent state 'z'\n",
    "        self.latent_head = nn.Linear(d_model, d_model)\n",
    "        # This head refines the answer 'y'\n",
    "        self.answer_head = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        \"\"\"\n",
    "        x: The embedded input question/puzzle. Shape: (batch, seq_len, d_model)\n",
    "        y: The current embedded prediction. Shape: (batch, seq_len, d_model)\n",
    "        z: The current latent reasoning state. Shape: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Concatenate inputs along the feature dimension\n",
    "        combined_input = torch.cat((x, y, z), dim=-1)\n",
    "        \n",
    "        # Project the combined input to the model's dimension\n",
    "        projected_input = self.input_proj(combined_input)\n",
    "\n",
    "        # Pass through the Transformer encoder\n",
    "        transformer_output = self.transformer_encoder(projected_input)\n",
    "\n",
    "        # As per the paper's logic, the model can update both y and z.\n",
    "        # We'll create two separate outputs from the transformer result.\n",
    "        \n",
    "        # The logic is: given x, y, z -> produce a new z'\n",
    "        # And given y, z -> produce a new y'\n",
    "        \n",
    "        # For simplicity and following Figure 1, we will have two heads.\n",
    "        # The main output of the transformer will be used to update the latent 'z'\n",
    "        new_z = self.latent_head(transformer_output)\n",
    "\n",
    "        # The paper suggests the answer update step uses the (new) latent z and old y.\n",
    "        # We will model this by feeding the transformer output and old 'y' to the answer head.\n",
    "        new_y = self.answer_head(transformer_output + y) # Using a residual connection for stability\n",
    "\n",
    "        return new_y, new_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 3: Defining the Recursive Logic and Training Functions\n",
    "This is the core logic from the paper's pseudocode. We'll implement latent_recursion for the inner loop and deep_recursion for the outer loop, which cleverly uses torch.no_grad() to manage memory and mimic the paper's training strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:56:48.830262Z",
     "iopub.status.busy": "2025-10-14T00:56:48.829779Z",
     "iopub.status.idle": "2025-10-14T00:56:48.837260Z",
     "shell.execute_reply": "2025-10-14T00:56:48.836553Z",
     "shell.execute_reply.started": "2025-10-14T00:56:48.830236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def latent_recursion(model, x, y, z, n=N_RECURSION):\n",
    "    \"\"\"\n",
    "    Performs the inner 'latent reasoning' loop.\n",
    "    'n' recursive steps to refine the latent state 'z'.\n",
    "    \"\"\"\n",
    "    for _ in range(n):\n",
    "        # In this simplified model, both y and z are returned.\n",
    "        # The paper's core idea is that the latent state 'z' is what's primarily refined here.\n",
    "        _, z = model(x, y, z)\n",
    "    \n",
    "    # After refining z, one final step to refine y\n",
    "    y, z = model(x, y, z)\n",
    "    \n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(model, x, y, z, n=N_RECURSION, T=T_RECURSION):\n",
    "    \"\"\"\n",
    "    Performs one step of the outer 'deep supervision' loop.\n",
    "    This involves T-1 steps with no gradients and one final step with gradients.\n",
    "    \"\"\"\n",
    "    # Recurse for T-1 steps without tracking gradients to save memory\n",
    "    with torch.no_grad():\n",
    "        for _ in range(T - 1):\n",
    "            y, z = latent_recursion(model, x, y, z, n)\n",
    "            \n",
    "    # Perform the final recursion step with gradients enabled\n",
    "    y, z = latent_recursion(model, x, y, z, n)\n",
    "    \n",
    "    return y, z\n",
    "\n",
    "# We also need an output head to convert our embedding back to vocabulary tokens\n",
    "class OutputHead(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, y_embedding):\n",
    "        return self.linear(y_embedding)\n",
    "\n",
    "# And a Q-head for the Adaptive Computation Time (ACT) halting mechanism\n",
    "class QHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, y_embedding):\n",
    "        # We just need a single scalar value for the halt probability\n",
    "        # We take the mean over the sequence length dimension\n",
    "        return self.linear(y_embedding.mean(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 4: Data Preparation\n",
    "In this batch, we will:\n",
    "Load a Sudoku dataset from the Hugging Face Hub.\n",
    "Create a simple tokenizer to convert the puzzle strings into numerical tokens.\n",
    "Set up a custom PyTorch Dataset to handle the tokenization.\n",
    "Create DataLoaders to feed the data to our model in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:56:52.379829Z",
     "iopub.status.busy": "2025-10-14T00:56:52.379559Z",
     "iopub.status.idle": "2025-10-14T00:56:53.385653Z",
     "shell.execute_reply": "2025-10-14T00:56:53.384776Z",
     "shell.execute_reply.started": "2025-10-14T00:56:52.379811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Features: {'source': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'rating': Value(dtype='int64', id=None)}\n",
      "\n",
      "First Training Example: {'source': 'puzzles4_forum_hardest_1905', 'question': '5...27..9..41......1..5.3...92.6.8...5......66..7..29.8...7...2.......8...9..36..', 'answer': '583427169974136528216859374792364851351298746648715293865971432137642985429583617', 'rating': 18}\n"
     ]
    }
   ],
   "source": [
    "# --- DATA LOADING ---\n",
    "dataset_name = \"sapientinc/sudoku-extreme\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Define Vocabulary\n",
    "VOCAB = ['.'] + [str(i) for i in range(1, 10)] # The dataset uses '.' for empty cells\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "token_to_id = {token: i for i, token in enumerate(VOCAB)}\n",
    "id_to_token = {i: token for i, token in enumerate(VOCAB)}\n",
    "SEQ_LEN = 81\n",
    "\n",
    "# Let's inspect the actual structure of the loaded dataset to be sure.\n",
    "print(\"Dataset Features:\", dataset['train'].features)\n",
    "print(\"\\nFirst Training Example:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:56:57.165226Z",
     "iopub.status.busy": "2025-10-14T00:56:57.164929Z",
     "iopub.status.idle": "2025-10-14T00:56:57.187544Z",
     "shell.execute_reply": "2025-10-14T00:56:57.186901Z",
     "shell.execute_reply.started": "2025-10-14T00:56:57.165206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quiz batch shape: torch.Size([16, 81])\n",
      "Solution batch shape: torch.Size([16, 81])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SudokuDataset(Dataset):\n",
    "    def __init__(self, data, token_map):\n",
    "        self.data = data\n",
    "        self.token_map = token_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # --- FINAL CORRECTION ---\n",
    "        # Using the exact column names you found: 'question' and 'answer'\n",
    "        # Also, the dataset uses '.' for empty cells, not '0'.\n",
    "        quiz_str = item['question']\n",
    "        solution_str = item['answer']\n",
    "        \n",
    "        # Convert strings to lists of integer token IDs\n",
    "        quiz_tokens = torch.tensor([self.token_map[char] for char in quiz_str], dtype=torch.long)\n",
    "        solution_tokens = torch.tensor([self.token_map[char] for char in solution_str], dtype=torch.long)\n",
    "        \n",
    "        return quiz_tokens, solution_tokens\n",
    "\n",
    "# Create the datasets.\n",
    "# The paper trains on only 1K samples, so let's use .select() for efficiency.\n",
    "train_data = SudokuDataset(dataset['train'].select(range(1000)), token_to_id)\n",
    "test_data = SudokuDataset(dataset['test'].select(range(1000)), token_to_id) # Using a 1k slice of test for quick eval\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Let's inspect a single batch to confirm the shape\n",
    "try:\n",
    "    sample_quiz, sample_solution = next(iter(train_dataloader))\n",
    "    print(f\"\\nQuiz batch shape: {sample_quiz.shape}\")\n",
    "    print(f\"Solution batch shape: {sample_solution.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nFailed to create a batch. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 5: Initializing Models and Optimizer\n",
    "Now we have our model architecture and our data loaders. The final step before training is to instantiate all the necessary components:\n",
    "Embedding Layer: A layer to convert our numerical tokens into dense vectors (the d_model dimension).\n",
    "TRM Model: Our main recursive network.\n",
    "Output Head: To convert the model's output embeddings back into token probabilities.\n",
    "Q-Head: For the halting mechanism.\n",
    "Optimizer: To update the model's weights during training.\n",
    "Loss Function: To measure the difference between the model's predictions and the true solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:57:00.925242Z",
     "iopub.status.busy": "2025-10-14T00:57:00.924942Z",
     "iopub.status.idle": "2025-10-14T00:57:07.755435Z",
     "shell.execute_reply": "2025-10-14T00:57:07.754786Z",
     "shell.execute_reply.started": "2025-10-14T00:57:00.925221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Attempting Hugging Face Login ---\n",
      "✅ Successfully logged into Hugging Face.\n",
      "\n",
      "--- Attempting Weights & Biases Login ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully logged into Weights & Biases.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251014_005701-cnmaputq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq' target=\"_blank\">brisk-sea-1</a></strong> to <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ W&B run initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# : Login to Hugging Face and Weights & Biases\n",
    "# ==============================================================================\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# --- Hugging Face Login ---\n",
    "print(\"--- Attempting Hugging Face Login ---\")\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    hf_token = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n",
    "    login(token=hf_token)\n",
    "    print(\"✅ Successfully logged into Hugging Face.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not log into Hugging Face. Please ensure the 'HUGGINGFACE_API_KEY' secret is set.\")\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# --- Weights & Biases Login ---\n",
    "print(\"\\n--- Attempting Weights & Biases Login ---\")\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"✅ Successfully logged into Weights & Biases.\")\n",
    "    \n",
    "    # --- Initialize W&B Run ---\n",
    "    # This should happen right after a successful login\n",
    "    run = wandb.init(\n",
    "        project=\"tiny-recursive-model-sudoku-v1\",\n",
    "        config={\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"epochs\": NUM_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"d_model\": D_MODEL,\n",
    "            \"n_layers\": N_LAYERS,\n",
    "            \"dim_feedforward\": DIM_FEEDFORWARD,\n",
    "            \"n_recursion\": N_RECURSION,\n",
    "            \"t_recursion\": T_RECURSION,\n",
    "        },\n",
    "    )\n",
    "    print(\"✅ W&B run initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Could not log into W&B or initialize run. Please ensure the 'wandb_api_key' secret is set.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:57:15.257909Z",
     "iopub.status.busy": "2025-10-14T00:57:15.257646Z",
     "iopub.status.idle": "2025-10-14T00:57:15.328092Z",
     "shell.execute_reply": "2025-10-14T00:57:15.327548Z",
     "shell.execute_reply.started": "2025-10-14T00:57:15.257891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Token Embedding Layer\n",
    "embedding_layer = nn.Embedding(VOCAB_SIZE, D_MODEL).to(device)\n",
    "\n",
    "# 2. The main Tiny Recursive Model\n",
    "# --- MEMORY OPTIMIZATION ---\n",
    "# Pass our smaller dim_feedforward to the model constructor\n",
    "trm_model = TinyRecursiveModel(\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD\n",
    ").to(device)\n",
    "\n",
    "# 3. The Output Head\n",
    "output_head = OutputHead(D_MODEL, VOCAB_SIZE).to(device)\n",
    "\n",
    "# 4. The Q-Head\n",
    "q_head = QHead(D_MODEL).to(device)\n",
    "\n",
    "# Combine all model parameters for the optimizer\n",
    "all_params = (\n",
    "    list(embedding_layer.parameters()) +\n",
    "    list(trm_model.parameters()) +\n",
    "    list(output_head.parameters()) +\n",
    "    list(q_head.parameters())\n",
    ")\n",
    "\n",
    "# 5. Optimizer\n",
    "optimizer = AdamW(all_params, lr=LEARNING_RATE)\n",
    "\n",
    "# 6. Loss Functions\n",
    "prediction_loss_fn = nn.CrossEntropyLoss()\n",
    "act_loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 6: The Training Loop\n",
    "This is where we put everything together. The loop will iterate through our training data for a few epochs. Inside the loop, we implement the full \"Deep Supervision\" logic as described in the paper's pseudocode.\n",
    "Initialize States: For each puzzle, we start with initial y and z embeddings. We'll use simple zero tensors for this.\n",
    "Deep Supervision Loop: We loop for N_SUP steps.\n",
    "Forward Pass: In each step, we call our deep_recursion function.\n",
    "Calculate Losses:\n",
    "Calculate the prediction loss between the model's output and the true solution.\n",
    "Calculate the ACT (halting) loss. The paper's pseudocode suggests the target is 1 if the prediction is correct and 0 otherwise.\n",
    "Backpropagation: We compute the gradients and update the model weights.\n",
    "Detach: Crucially, we detach the y and z states from the computation graph before the next supervision step. This is the key to managing memory.\n",
    "ACT Halting: We check the halt condition from the Q-head to potentially break the loop early, saving computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T01:04:06.160422Z",
     "iopub.status.busy": "2025-10-14T01:04:06.159818Z",
     "iopub.status.idle": "2025-10-14T01:14:30.195763Z",
     "shell.execute_reply": "2025-10-14T01:14:30.194944Z",
     "shell.execute_reply.started": "2025-10-14T01:04:06.160399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [05:11<00:00,  4.94s/it, avg_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Average Training Loss: 1.5275759034686618\n",
      "--- Epoch 2/2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [05:12<00:00,  4.95s/it, avg_loss=1.51]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Average Training Loss: 1.5147268147695632\n",
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▇▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>1.4997</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-sea-1</strong> at: <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku/runs/cnmaputq</a><br> View project at: <a href='https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/tiny-recursive-model-sudoku</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251014_005701-cnmaputq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# BATCH 6: TRAINING & EVALUATION LOOP with W&B\n",
    "# ==============================================================================\n",
    "\n",
    "# First, ensure the evaluate function is defined OUTSIDE the main training loop\n",
    "# so we can call it repeatedly.\n",
    "def evaluate(embedding_layer, model, output_head, test_loader, device):\n",
    "    \"\"\"Evaluates the model on the test dataset.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    total_puzzles = 0\n",
    "    correct_puzzles = 0\n",
    "    total_digits = 0\n",
    "    correct_digits = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for quiz_tokens, solution_tokens in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            quiz_tokens, solution_tokens = quiz_tokens.to(device), solution_tokens.to(device)\n",
    "\n",
    "            x_embedded = embedding_layer(quiz_tokens)\n",
    "            y_embedded = torch.zeros_like(x_embedded)\n",
    "            z_embedded = torch.zeros_like(x_embedded)\n",
    "            \n",
    "            # At test time, run the full N_SUP recursion steps\n",
    "            for _ in range(N_SUP):\n",
    "                y_refined, z_refined = deep_recursion(model, x_embedded, y_embedded, z_embedded)\n",
    "                y_embedded, z_embedded = y_refined.detach(), z_refined.detach()\n",
    "            \n",
    "            # Get the final prediction after all steps\n",
    "            y_logits = output_head(y_embedded)\n",
    "            final_y_pred_tokens = torch.argmax(y_logits, dim=-1)\n",
    "\n",
    "            correct_puzzles += torch.all(final_y_pred_tokens == solution_tokens, dim=1).sum().item()\n",
    "            total_puzzles += quiz_tokens.size(0)\n",
    "            correct_digits += (final_y_pred_tokens == solution_tokens).sum().item()\n",
    "            total_digits += quiz_tokens.numel()\n",
    "\n",
    "    puzzle_accuracy = (correct_puzzles / total_puzzles) * 100\n",
    "    digit_accuracy = (correct_digits / total_digits) * 100\n",
    "    \n",
    "    return puzzle_accuracy, digit_accuracy\n",
    "\n",
    "# --- MAIN TRAINING LOOP ---\n",
    "NUM_EPOCHS = 20\n",
    "wandb.watch(trm_model, log=\"all\", log_freq=100)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # --- TRAINING PHASE ---\n",
    "    trm_model.train()\n",
    "    batch_losses = [] # Store all batch losses for the epoch\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    \n",
    "    for i, (quiz_tokens, solution_tokens) in enumerate(progress_bar):\n",
    "        quiz_tokens, solution_tokens = quiz_tokens.to(device), solution_tokens.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_embedded = embedding_layer(quiz_tokens)\n",
    "        y_embedded = torch.zeros_like(x_embedded)\n",
    "        z_embedded = torch.zeros_like(x_embedded)\n",
    "        total_loss = 0\n",
    "        actual_steps = 0\n",
    "\n",
    "        for step in range(N_SUP):\n",
    "            actual_steps += 1\n",
    "            y_refined, z_refined = deep_recursion(trm_model, x_embedded, y_embedded, z_embedded)\n",
    "            y_logits = output_head(y_refined)\n",
    "            q_value = q_head(y_refined)\n",
    "\n",
    "            prediction_loss = prediction_loss_fn(y_logits.view(-1, VOCAB_SIZE), solution_tokens.view(-1))\n",
    "            with torch.no_grad():\n",
    "                y_pred_tokens = torch.argmax(y_logits, dim=-1)\n",
    "                is_correct = torch.all(y_pred_tokens == solution_tokens, dim=1).float()\n",
    "            halt_loss = act_loss_fn(q_value.squeeze(), is_correct)\n",
    "            step_loss = prediction_loss + halt_loss\n",
    "            total_loss += step_loss\n",
    "\n",
    "            y_embedded, z_embedded = y_refined.detach(), z_refined.detach()\n",
    "\n",
    "            if torch.sigmoid(q_value).mean() > 0.9 and step > 0:\n",
    "                break\n",
    "        \n",
    "        avg_loss = total_loss / actual_steps\n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(avg_loss.item())\n",
    "        progress_bar.set_postfix({\"current_batch_loss\": avg_loss.item()})\n",
    "\n",
    "    # --- LOGGING PHASE (after each epoch) ---\n",
    "    # Calculate mean and median loss for the epoch\n",
    "    avg_epoch_loss = np.mean(batch_losses)\n",
    "    median_epoch_loss = np.median(batch_losses)\n",
    "    print(f\"Epoch {epoch+1} finished. Avg Loss: {avg_epoch_loss:.4f}, Median Loss: {median_epoch_loss:.4f}\")\n",
    "    \n",
    "    # --- EVALUATION PHASE (after each epoch) ---\n",
    "    print(\"Running evaluation...\")\n",
    "    puzzle_acc, digit_acc = evaluate(embedding_layer, trm_model, output_head, test_dataloader, device)\n",
    "    print(f\"Evaluation Results - Puzzle Acc: {puzzle_acc:.2f}%, Digit Acc: {digit_acc:.2f}%\")\n",
    "\n",
    "    # Log all metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"avg_train_loss\": avg_epoch_loss,\n",
    "        \"median_train_loss\": median_epoch_loss, # <-- NEW\n",
    "        \"puzzle_accuracy\": puzzle_acc,\n",
    "        \"digit_accuracy\": digit_acc,\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Training Finished! ---\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:17:23.928716Z",
     "iopub.status.busy": "2025-10-14T00:17:23.928306Z",
     "iopub.status.idle": "2025-10-14T00:32:22.391271Z",
     "shell.execute_reply": "2025-10-14T00:32:22.390349Z",
     "shell.execute_reply.started": "2025-10-14T00:17:23.928695Z"
    }
   },
   "source": [
    "NUM_EPOCHS = 3 # Let's start with 3 epochs for a quick test run.\n",
    "\n",
    "# Set models to training mode\n",
    "embedding_layer.train()\n",
    "trm_model.train()\n",
    "output_head.train()\n",
    "q_head.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for quiz_tokens, solution_tokens in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        quiz_tokens = quiz_tokens.to(device)\n",
    "        solution_tokens = solution_tokens.to(device)\n",
    "        \n",
    "        # --- Start of Deep Supervision ---\n",
    "        \n",
    "        # 1. Initialize states for the deep supervision loop\n",
    "        # Embed the input puzzle once per supervision cycle\n",
    "        x_embedded = embedding_layer(quiz_tokens)\n",
    "\n",
    "        # Initialize y (prediction) and z (latent) embeddings as zero tensors\n",
    "        # The paper calls this y_init and z_init\n",
    "        y_embedded = torch.zeros_like(x_embedded, device=device)\n",
    "        z_embedded = torch.zeros_like(x_embedded, device=device)\n",
    "\n",
    "        # Reset gradients for the new supervision cycle\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss = 0\n",
    "\n",
    "        for step in range(N_SUP):\n",
    "            # 2. Perform one full deep recursion step (T-1 no-grad, 1 with-grad)\n",
    "            y_refined, z_refined = deep_recursion(trm_model, x_embedded, y_embedded, z_embedded)\n",
    "            \n",
    "            # Get the model's prediction logits and halting value\n",
    "            y_logits = output_head(y_refined)\n",
    "            q_value = q_head(y_refined) # Halting probability logit\n",
    "\n",
    "            # 3. Calculate Losses\n",
    "            # Reshape for CrossEntropyLoss: (Batch * SeqLen, VocabSize)\n",
    "            prediction_loss = prediction_loss_fn(\n",
    "                y_logits.view(-1, VOCAB_SIZE),\n",
    "                solution_tokens.view(-1)\n",
    "            )\n",
    "\n",
    "            # ACT halting loss\n",
    "            with torch.no_grad():\n",
    "                # Get the actual predictions by finding the max logit\n",
    "                y_pred_tokens = torch.argmax(y_logits, dim=-1)\n",
    "                # The halt target is 1 if the entire puzzle is correct, 0 otherwise.\n",
    "                is_correct = torch.all(y_pred_tokens == solution_tokens, dim=1).float()\n",
    "            \n",
    "            # The paper's pseudocode uses a simple BCE loss for halting\n",
    "            halt_loss = act_loss_fn(q_value.squeeze(), is_correct)\n",
    "\n",
    "            # Combine losses\n",
    "            step_loss = prediction_loss + halt_loss\n",
    "            total_loss += step_loss\n",
    "\n",
    "            # 4. Detach states for the next iteration (THIS IS THE KEY MEMORY SAVING STEP)\n",
    "            y_embedded = y_refined.detach()\n",
    "            z_embedded = z_refined.detach()\n",
    "\n",
    "            # 5. ACT Early Stopping\n",
    "            # If the model is confident enough to halt, we stop the supervision loop for this batch.\n",
    "            # We use sigmoid to convert the logit to a probability\n",
    "            if torch.sigmoid(q_value).mean() > 0.9 and step > 0: # Check if average halt prob is high\n",
    "                break\n",
    "        \n",
    "        # 6. Backpropagation\n",
    "        # The accumulated loss from all steps is backpropagated at once.\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished. Last batch total loss: {total_loss.item()}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T00:32:22.392477Z",
     "iopub.status.busy": "2025-10-14T00:32:22.392197Z",
     "iopub.status.idle": "2025-10-14T00:35:13.439134Z",
     "shell.execute_reply": "2025-10-14T00:35:13.438305Z",
     "shell.execute_reply.started": "2025-10-14T00:32:22.392453Z"
    }
   },
   "source": [
    "def evaluate(embedding_layer, model, output_head, test_loader, device):\n",
    "    embedding_layer.eval()\n",
    "    model.eval()\n",
    "    output_head.eval()\n",
    "    q_head.eval() # Also set q_head to eval mode\n",
    "\n",
    "    total_puzzles = 0\n",
    "    correct_puzzles = 0\n",
    "    total_digits = 0\n",
    "    correct_digits = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for quiz_tokens, solution_tokens in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            quiz_tokens = quiz_tokens.to(device)\n",
    "            solution_tokens = solution_tokens.to(device)\n",
    "\n",
    "            # --- Evaluation follows the same deep recursion logic ---\n",
    "            x_embedded = embedding_layer(quiz_tokens)\n",
    "            y_embedded = torch.zeros_like(x_embedded, device=device)\n",
    "            z_embedded = torch.zeros_like(x_embedded, device=device)\n",
    "            \n",
    "            final_y_pred_tokens = None\n",
    "\n",
    "            # The paper uses the full N_sup steps at test time\n",
    "            for step in range(N_SUP):\n",
    "                y_refined, z_refined = deep_recursion(model, x_embedded, y_embedded, z_embedded)\n",
    "                y_embedded, z_embedded = y_refined.detach(), z_refined.detach()\n",
    "                \n",
    "                # We can check the final prediction after all steps\n",
    "                if step == N_SUP - 1:\n",
    "                    y_logits = output_head(y_refined)\n",
    "                    final_y_pred_tokens = torch.argmax(y_logits, dim=-1)\n",
    "\n",
    "            # Compare the final prediction with the solution\n",
    "            correct_puzzles_batch = torch.all(final_y_pred_tokens == solution_tokens, dim=1).sum().item()\n",
    "            correct_digits_batch = (final_y_pred_tokens == solution_tokens).sum().item()\n",
    "            \n",
    "            correct_puzzles += correct_puzzles_batch\n",
    "            total_puzzles += quiz_tokens.size(0)\n",
    "            correct_digits += correct_digits_batch\n",
    "            total_digits += quiz_tokens.size(0) * SEQ_LEN\n",
    "\n",
    "    puzzle_accuracy = (correct_puzzles / total_puzzles) * 100\n",
    "    digit_accuracy = (correct_digits / total_digits) * 100\n",
    "    \n",
    "    return puzzle_accuracy, digit_accuracy\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "print(\"\\nStarting evaluation on the test set...\")\n",
    "puzzle_acc, digit_acc = evaluate(embedding_layer, trm_model, output_head, test_dataloader, device)\n",
    "\n",
    "print(f\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Puzzle Accuracy: {puzzle_acc:.2f}%\")\n",
    "print(f\"Digit Accuracy: {digit_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
